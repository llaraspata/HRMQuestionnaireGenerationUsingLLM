{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('\\\\'.join(os.getcwd().split('\\\\')[:-1])+'\\\\src')\n",
    "\n",
    "from src.visualization.PairResultVisualizer import PairResultVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(sentence):\n",
    "    \"\"\"\n",
    "    Counts the number of words in a sentence, ignoring punctuation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "        splitted = sentence.split()\n",
    "        return len(splitted)\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_length_distribution_comparison(df_0s, df_1s, column, name, element):\n",
    "    \"\"\"\n",
    "        Plots the distribution of the length of the reviews for zero and one shot predictions.\n",
    "    \"\"\"\n",
    "    lens_0s = [count_words(sentence) for sentence in df_0s[df_0s[column].notna()][column]]\n",
    "    lens_1s = [count_words(sentence) for sentence in df_1s[df_1s[column].notna()][column]]\n",
    "    \n",
    "    mean_lens_0s = np.mean(lens_0s)\n",
    "    max_lens_0s = np.max(lens_0s)\n",
    "    min_lens_0s = np.min(lens_0s)\n",
    "\n",
    "    mean_lens_1s = np.mean(lens_1s)\n",
    "    max_lens_1s = np.max(lens_1s)\n",
    "    min_lens_1s = np.min(lens_1s)\n",
    "\n",
    "    title = f\"\"\"{name} - {element} length distribution \n",
    "    <br>\n",
    "    0s -> [Mean: {int(round(mean_lens_0s))}, Max: {max_lens_0s}, Min: {min_lens_0s}]    1s -> [Mean: {int(round(mean_lens_1s))}, Max: {max_lens_1s}, Min: {min_lens_1s}]\"\"\"\n",
    "\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Histogram(x=lens_0s, name=\"Zero-Shot\", marker_color=\"#D0006F\", histnorm='probability'))\n",
    "    fig.add_trace(go.Histogram(x=lens_1s, name=\"One-Shot\", marker_color=\"#24135F\", histnorm='probability'))\n",
    "\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"{name} - {element} length distribution\",\n",
    "        xaxis_title=\"Length\",\n",
    "        yaxis_title=\"Frequency\"\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_length_distribution_comparison_with_ref(df_ref, df_0s, df_1s, column, name, element):\n",
    "    \"\"\"\n",
    "        Plots the distribution of the length of the reviews for zero and one shot predictions.\n",
    "    \"\"\"\n",
    "    lens_ref = [count_words(sentence) for sentence in df_ref[df_ref[column].notna()][column]]\n",
    "    lens_0s = [count_words(sentence) for sentence in df_0s[df_0s[column].notna()][column]]\n",
    "    lens_1s = [count_words(sentence) for sentence in df_1s[df_1s[column].notna()][column]]\n",
    "\n",
    "    mean_lens_ref = np.mean(lens_ref)\n",
    "    max_lens_ref = np.max(lens_ref)\n",
    "    min_lens_ref = np.min(lens_ref)\n",
    "\n",
    "    mean_lens_0s = np.mean(lens_0s)\n",
    "    max_lens_0s = np.max(lens_0s)\n",
    "    min_lens_0s = np.min(lens_0s)\n",
    "\n",
    "    mean_lens_1s = np.mean(lens_1s)\n",
    "    max_lens_1s = np.max(lens_1s)\n",
    "    min_lens_1s = np.min(lens_1s)\n",
    "\n",
    "    title = f\"\"\"{name} - {element} length distribution \n",
    "    <br>\n",
    "    GT -> [Mean: {int(round(mean_lens_ref))}, Max: {max_lens_ref}, Min: {min_lens_ref}]    0s -> [Mean: {int(round(mean_lens_0s))}, Max: {max_lens_0s}, Min: {min_lens_0s}]    1s -> [Mean: {int(round(mean_lens_1s))}, Max: {max_lens_1s}, Min: {min_lens_1s}]\"\"\"\n",
    "\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Histogram(x=lens_ref, name=\"Human-written\", marker_color=\"#89A9EE\", histnorm='probability'))\n",
    "    fig.add_trace(go.Histogram(x=lens_0s, name=\"Zero-Shot\", marker_color=\"#D0006F\", histnorm='probability'))\n",
    "    fig.add_trace(go.Histogram(x=lens_1s, name=\"One-Shot\", marker_color=\"#24135F\", histnorm='probability'))\n",
    "\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"{name} - {element} length distribution\",\n",
    "        xaxis_title=\"Length\",\n",
    "        yaxis_title=\"Frequency\"\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "config_path = os.path.join(os.path.abspath(os.path.join(os.getcwd(), os.pardir)), \"src\", \"visualization\", \"experiment_pairs.json\")\n",
    "with open(config_path, \"r\") as f:\n",
    "    experiment_confs = json.load(f)\n",
    "f.close()\n",
    "\n",
    "pairs = experiment_confs[\"pairs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), os.pardir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_preds = PairResultVisualizer(pairs[0], project_root=PROJECT_ROOT)\n",
    "pairs.remove(pairs[0])\n",
    "\n",
    "pairs_to_remove = []\n",
    "\n",
    "for pair in pairs:\n",
    "    if pair[\"id\"].__contains__(\"GPT-4\"):\n",
    "        gpt_4_preds.add_pair(pair)\n",
    "        gpt_4_preds.load_data(project_root=PROJECT_ROOT)\n",
    "        \n",
    "        pairs_to_remove.append(pair)\n",
    "\n",
    "for pair in pairs_to_remove:\n",
    "    pairs.remove(pair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_35_preds = PairResultVisualizer(pairs[0], project_root=PROJECT_ROOT)\n",
    "pairs.remove(pairs[0])\n",
    "\n",
    "for pair in pairs:\n",
    "    gpt_35_preds.add_pair(pair)\n",
    "    gpt_35_preds.load_data(project_root=PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Questions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_length_distribution_comparison_with_ref(gpt_35_preds.ground_truth_questions, gpt_35_preds.zero_shot_full_questions, gpt_35_preds.one_shot_full_questions, \"NAME\", \"GPT-3.5-Turbo [Task 1]\", \"Question\")\n",
    "plot_length_distribution_comparison_with_ref(gpt_35_preds.ground_truth_questions, gpt_35_preds.zero_shot_questions, gpt_35_preds.one_shot_questions, \"NAME\", \"GPT-3.5-Turbo [Task 2]\", \"Question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_length_distribution_comparison_with_ref(gpt_4_preds.ground_truth_questions, gpt_4_preds.zero_shot_full_questions, gpt_4_preds.one_shot_full_questions, \"NAME\", \"GPT-4-Turbo [Task 1]\", \"Question\")\n",
    "plot_length_distribution_comparison_with_ref(gpt_4_preds.ground_truth_questions, gpt_4_preds.zero_shot_questions, gpt_4_preds.one_shot_questions, \"NAME\", \"GPT-4-Turbo [Task 2]\", \"Question\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Answers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_length_distribution_comparison_with_ref(gpt_35_preds.ground_truth_answers, gpt_35_preds.zero_shot_full_answers, gpt_35_preds.one_shot_full_answers, \"ANSWER\", \"GPT-3.5-Turbo [Task 1]\", \"Answer\")\n",
    "plot_length_distribution_comparison_with_ref(gpt_35_preds.ground_truth_answers, gpt_35_preds.zero_shot_answers, gpt_35_preds.one_shot_answers, \"ANSWER\", \"GPT-3.5-Turbo [Task 2]\", \"Answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_length_distribution_comparison_with_ref(gpt_4_preds.ground_truth_answers, gpt_4_preds.zero_shot_full_answers, gpt_4_preds.one_shot_full_answers, \"ANSWER\", \"GPT-4-Turbo [Task 1]\", \"Answer\")\n",
    "plot_length_distribution_comparison_with_ref(gpt_4_preds.ground_truth_answers, gpt_4_preds.zero_shot_answers, gpt_4_preds.one_shot_answers, \"ANSWER\", \"GPT-4-Turbo [Task 2]\", \"Answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Distribution (M, V)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_distribution(df, model, tech, task, column):    \n",
    "    lens = [count_words(sentence) for sentence in df[df[column].notna()][column]]\n",
    "\n",
    "    mean = int(round(np.mean(lens)))\n",
    "    var = int(round(np.var(lens)))\n",
    "    max_l = int(round(np.max(lens)))\n",
    "    min_l = int(round(np.min(lens)))\n",
    "\n",
    "    print(f\"{model} & {tech} & {task} & {mean} & {var} & {max_l} & {min_l} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human-written &  &  & 13 & 30 & 34 & 1 \n",
      "GPT-3.5-Turbo & Zero-Shot & 1 & 12 & 12 & 27 & 4 \n",
      "GPT-3.5-Turbo & One-Shot & 1 & 13 & 23 & 36 & 0 \n",
      "GPT-3.5-Turbo & Zero-Shot & 2 & 12 & 15 & 59 & 4 \n",
      "GPT-3.5-Turbo & One-Shot & 2 & 13 & 24 & 37 & 0 \n",
      "GPT-4-Turbo & Zero-Shot & 1 & 12 & 11 & 25 & 0 \n",
      "GPT-4-Turbo & One-Shot & 1 & 12 & 11 & 33 & 0 \n",
      "GPT-4-Turbo & Zero-Shot & 2 & 12 & 11 & 29 & 0 \n",
      "GPT-4-Turbo & One-Shot & 2 & 12 & 12 & 31 & 0 \n"
     ]
    }
   ],
   "source": [
    "print_distribution(gpt_35_preds.ground_truth_questions, \"Human-written\", \"\", \"\", \"NAME\")\n",
    "print_distribution(gpt_35_preds.zero_shot_full_questions, \"GPT-3.5-Turbo\", \"Zero-Shot\", \"1\", \"NAME\")\n",
    "print_distribution(gpt_35_preds.one_shot_full_questions, \"GPT-3.5-Turbo\", \"One-Shot\",\"1\", \"NAME\")\n",
    "print_distribution(gpt_35_preds.zero_shot_questions, \"GPT-3.5-Turbo\", \"Zero-Shot\", \"2\", \"NAME\")\n",
    "print_distribution(gpt_35_preds.one_shot_questions, \"GPT-3.5-Turbo\", \"One-Shot\",\"2\", \"NAME\")\n",
    "print_distribution(gpt_4_preds.zero_shot_full_questions, \"GPT-4-Turbo\", \"Zero-Shot\", \"1\", \"NAME\")\n",
    "print_distribution(gpt_4_preds.one_shot_full_questions, \"GPT-4-Turbo\", \"One-Shot\",\"1\", \"NAME\")\n",
    "print_distribution(gpt_4_preds.zero_shot_questions, \"GPT-4-Turbo\", \"Zero-Shot\", \"2\", \"NAME\")\n",
    "print_distribution(gpt_4_preds.one_shot_questions, \"GPT-4-Turbo\", \"One-Shot\",\"2\", \"NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human-written &  &  & 2 & 2 & 26 & 1 \n",
      "GPT-3.5-Turbo & Zero-Shot & 1 & 1 & 1 & 12 & 0 \n",
      "GPT-3.5-Turbo & One-Shot & 1 & 2 & 2 & 14 & 0 \n",
      "GPT-3.5-Turbo & Zero-Shot & 2 & 1 & 1 & 10 & 0 \n",
      "GPT-3.5-Turbo & One-Shot & 2 & 2 & 2 & 20 & 0 \n",
      "GPT-4-Turbo & Zero-Shot & 1 & 2 & 1 & 14 & 0 \n",
      "GPT-4-Turbo & One-Shot & 1 & 2 & 1 & 20 & 0 \n",
      "GPT-4-Turbo & Zero-Shot & 2 & 2 & 2 & 13 & 0 \n",
      "GPT-4-Turbo & One-Shot & 2 & 2 & 1 & 12 & 0 \n"
     ]
    }
   ],
   "source": [
    "print_distribution(gpt_35_preds.ground_truth_answers, \"Human-written\", \"\", \"\", \"ANSWER\")\n",
    "print_distribution(gpt_35_preds.zero_shot_full_answers, \"GPT-3.5-Turbo\", \"Zero-Shot\", \"1\", \"ANSWER\")\n",
    "print_distribution(gpt_35_preds.one_shot_full_answers, \"GPT-3.5-Turbo\", \"One-Shot\",\"1\", \"ANSWER\")\n",
    "print_distribution(gpt_35_preds.zero_shot_answers, \"GPT-3.5-Turbo\", \"Zero-Shot\", \"2\", \"ANSWER\")\n",
    "print_distribution(gpt_35_preds.one_shot_answers, \"GPT-3.5-Turbo\", \"One-Shot\",\"2\", \"ANSWER\")\n",
    "\n",
    "print_distribution(gpt_4_preds.zero_shot_full_answers, \"GPT-4-Turbo\", \"Zero-Shot\", \"1\", \"ANSWER\")\n",
    "print_distribution(gpt_4_preds.one_shot_full_answers, \"GPT-4-Turbo\", \"One-Shot\",\"1\", \"ANSWER\")\n",
    "print_distribution(gpt_4_preds.zero_shot_answers, \"GPT-4-Turbo\", \"Zero-Shot\", \"2\", \"ANSWER\")\n",
    "print_distribution(gpt_4_preds.one_shot_answers, \"GPT-4-Turbo\", \"One-Shot\",\"2\", \"ANSWER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
